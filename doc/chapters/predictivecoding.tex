\section{Relation to Predictive Coding}

In this section, we consider an implementation of the message passing implicated by the HGF which is as close as possible to current proposals of neural architectures for predictive coding \cite{Shipp2016}.\\

We will separately consider \textsf{VAPE} and \textsf{VOPE} coupling and realize that the message passing for \textsf{VAPE} coupling is almost equivalent to the proposed PC architecture, while the \textsf{VOPE} coupling comes with more challenges, simply because it hasn't been addressed in detail in existing PC proposals on neural architecture.

\loadfigure[fig:vaall]{figures/vape_all}

\subsection{VAPE coupling}

Figure \ref{fig:vaall} sketches a possible architecture implementing all computations involved in \textsf{VAPE} coupling for three example cortical regions (levels). The assignment of neural elements to cortical layers follows the proposal in in figure 3 of \cite{Shipp2016}. For example, we've placed all precision-related nodes into the upper layers, while expectations and predictions of the mean live in intermediate and deep layers, respectively. In the following, we go through the different computational steps and note differences to the proposal for predictive coding.

For the \textsf{PE} step, we have the following computation:

\begin{equation}
	\delta_i^{(k)} = \mu_i^{(k)} - \hat{\mu}_i^{(k)}.
\end{equation}

\loadfigure[fig:vape]{figures/vape_pe}

The message passing implicated is depicted in figure \ref{fig:vape} and is in line with PE computations as suggested by \cite{Shipp2016}.

In the \textsf{Update} step, the posterior estimates of mean an variance are given by:

\begin{equation}
	\mu_i^{(k)} = \hat{\mu}_i^{(k)} + \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)}
\end{equation}

and

\begin{equation}
	\pi_i^{(k)} = \hat{\pi}_i^{(k)} + \alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}.
\end{equation}

A possible message passing implementation for these updates is shown in figure \ref{fig:vaupdate}. There are two important differences to \cite{Shipp2016} to note here.\\

\loadfigure[fig:vaupdate]{figures/vape_update}

First, in \textsf{PC} proposal, the update of the mean $\mu_i$ is driven both by the lower-level PE $\delta_{i-1}$ and the PE on the level itself, $\delta_i$, which mediates the influence of the empirical prior, or the prediction, on the mean. 

In the HGF equations, we do not have this negative feedback loop from $\delta_i$ to $\mu_i$. Instead, the prediction $\hat{\mu}_i$ has a direct influence on $\mu_i$ (see equations above), hence the arrow from the $\hat{\mu}_i$ node to the $\mu_i$ node in figure \ref{fig:vaupdate}. 

However, by considering the within-trial temporal evolution of the $\mu_i$ node, we can get the same feedback loop. Noting that $\mu_i$ as computed in the above equation will be the endpoint of the update, i.e., the equilibrium value to which the node should stabilize at the end of trial $k$, we can, in the style of \cite{Bogacz2017}, propose the following temporal evolution:

\begin{equation}
	\dot{\mu}_i^{(k)} = \hat{\mu}_i^{(k)} + \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \mu_i^{(k)}
\end{equation}

Clearly, a node with this dynamic converges to the required posterior value, which can be seen by setting $\dot{\mu}_i$ to zero. Now we note that

\begin{equation}
	\dot{\mu}_i^{(k)} = \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \mu_i^{(k)} + \hat{\mu}_i^{(k)}
\end{equation}

can be summarized as 

\begin{equation}
	\dot{\mu}_i^{(k)} = \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \delta_i^{(k)}.
\end{equation}

Following this logic, we could also place the arrow between $\delta_i$ and $\mu_i$. The exercise of writing down the within-trial temporal dynamics of the nodes will be repeated below for all nodes, which results in equations for within-trial network dynamics. These can be used to simulate within-trial responses of all nodes, examining network stability, and predicting neural dynamics which could for example be compared with electrophysiological data. 

The second difference to the \textsf{PC} proposal is rather irreducible. As already mentioned in the previous section, the messages sent up to parent nodes do not only consist of prediction errors or their precision weights. Instead, to update the precision node $\pi_i$, node $i$ also needs access to the prediction precision $\hat{\pi}_{i-1}$ from the level below. This is reflected in the violet arrow travelling from superficial layers of level $i$ to level $i+1$. 

Finally, in the \textsf{Prediction} step, we have:

\begin{equation}
	\hat{\mu}_i^{(k+1)} = \mu_i^{(k)} + \alpha_{i,i+1} \mu_{i+1}^{(k)}
\end{equation}

and

\begin{equation}
	\hat{\pi}_i^{(k+1)} = \frac{1}{\frac{1}{\pi_i^{(k)}} + \exp(\omega_i)}.
\end{equation}

\loadfigure[fig:vaprediction]{figures/vape_prediction}

This again seems unproblematic, although the arrow from $\mu_i$ to $\hat{\mu}_i$ would not appear in the classical \textsf{PC} architecture. Also, we've placed the predicted precision $\hat{\pi}$ in the superficial layers here, for convenience, as it interacts only with the posterior precision $\pi$ and acts as a weight on the prediction error, and also inaccordance with \cite{Shipp2016}, where precision signals arise from layers 2 and 3A. 

\subsection{VOPE coupling}
For volatility parents, we propose the following: Each level of a cortical hierarchy implements its own volatility parent in superficial layers. Coupling across layers is exclusively value-related. This means that having a volatility parent is nothing more but a more sophisticated way of predicting and updating a level's precision estimates. \\

This setup excludes volatility parents of volatility parents. However, it allows a cortical hierarchy where higher levels predict either the mean, or the volatility, of lower levels, both via value coupling. In other words, any given level receives top-down predictions not only about its mean, but also about its volatility, which is implemented as the mean of its volatility parent.\\

Due to this proposal, we slightly change the notation from here on. Level $i$ from now on refers to hierarchical level within a cortical hierarchy. Nodes on this level comprise the 'original' nodes $\mu_i$, $\pi_i$, $\hat{\mu}_i$, $\hat{\pi}_i$, and $\delta_i$, but also all the nodes belonging to a volatility parent of these nodes, which we now denote as $\check{\mu}_i$, $\check{\pi}_i$, $\hat{\check{\mu}}_i$, $\hat{\check{\pi}}_i$, and $\check{\delta}_i$. Here, $\check{\delta}_i$ refers to the value prediction error about the mean of the volatility, $\check{\mu}_i$. \\

We also introduce a new character for the \textsf{VOPE}, $\Delta_i$, which we write as a function of the \textsf{VAPE}, $\delta_i$, of level $i$. This PE is what communicates between a level's nodes and a level's volatility parent's nodes (see below for equations and visualizations). 

\loadfigure[fig:voall]{figures/vope_all}

Figure \ref{fig:voall} displays one proposal for this setup, where the volatility related nodes live within the superficial layers of each level. 

The volatility prediction error (\textsf{VOPE}) can be written as a function of the value prediction error (\textsf{VAPE}), where we now use $\delta_i$ only for \textsf{VAPE}s:

\begin{equation}
	\delta_i^{(k)} \equiv \delta_i^{(k, VAPE)} = \mu_i^{(k)} - \hat{\mu}_i^{(k)},
\end{equation}

We define the \textsf{VOPE} as 

\begin{equation}
  \begin{split}
    \Delta_i^{(k)} \equiv \delta_i^{(k, VOPE)} &= \frac{ \frac{1}{\pi_{i}^{(k)}} + (\mu_i^{(k)} - \hat{\mu}_i^{(k)})^2 }{ \frac{1}{\pi_{i}^{(k-1)}} + \nu_{i}^{(k)} } - 1 \\
    &= \hat{\pi}_i^{(k)} \left( \frac{1}{\pi_{i}^{(k)}} + (\mu_i^{(k)} - \hat{\mu}_i^{(k)})^2 \right) - 1 \\
    &= \hat{\pi}_i^{(k)} \left( \frac{1}{\pi_{i}^{(k)}} + (\delta_i^{(k)})^2 \right) - 1 \\
    &=  \frac{\hat{\pi}_i^{(k)}}{\pi_{i}^{(k)}} + \hat{\pi}_i^{(k)} (\delta_i^{(k)})^2 - 1. \\
  \end{split} 
\end{equation}

That means we are introducing a second prediction error unit $\Delta_i$ which is concerned with deviations from predicted uncertainty and is informed by value prediction errors and other estimates of uncertainty. It is this prediction error - a function of the unweighted (squared) value prediction error with a new precision weight - which communicates between a level's nodes and a level's volatility parent's nodes.\\

With the definition of $\Delta_i$ as given in the \textsf{PE} step and a new term of \textit{expected precision} $\gamma_i$ (defined for convenience in the \textsf{Prediction} step), the \textsf{Update} steps in volatility coupling become:

\begin{equation}
	\check{\mu}_i^{(k)} = \hat{\check{\mu}}_i^{(k)} + \frac{1}{2} \frac{\gamma_{i-1}^{(k)}}{\check{\pi}_i^{(k)}} \Delta_{i-1}^{(k)}
\end{equation}

\begin{equation}
	\check{\pi}_i^{(k)} = \hat{\check{\pi}}_i^{(k)} + \frac{1}{2} (\gamma_{i-1}^{(k)})^2 + (\gamma_{i-1}^{(k)})^2 \Delta_{i-1}^{(k)} - \frac{1}{2} \gamma_{i-1}^{(k)} \Delta_{i-1}^{(k)}
\end{equation}

Finally, In the \textsf{Prediction} step, we now need to compute four nodes: \\
the predicted (volatility) mean

\begin{equation}
	\hat{\check{\mu}}_i^{(k+1)} = \check{\mu}_i^{(k)},
\end{equation}

the precision of that prediction

\begin{equation}
  \hat{\check{\pi}}_i^{(k+1)} = \frac{1}{\frac{1}{\check{\pi}_i^{(k)}} + \nu_i^{(k+1)}}, 
\end{equation}

the predicted environmental uncertainty (as a function of the next higher level in the hierarchy, $\mu_{i+1}$)

\begin{equation}
  \nu_i^{(k+1)} = \exp(\kappa_{i,i+1} \mu_{i+1}^{(k)} + \omega_i),
\end{equation}

and the new (auxiliary) expected precision

\begin{equation}
  \gamma_i^{(k+1)} = \kappa_{i+1,i} \nu_i^{(k+1)} \hat{\check{\pi}}_i^{(k+1)}.
\end{equation}

The last node is only defined for convenience in terms of simplifying the equations and the corresponding message passing.
\section{Relation to Predictive Coding}

In this section, we consider an implementation of the message passing implicated by the HGF which is as close as possible to current proposals of neural architectures for predictive coding \cite{Shipp2016}.\\

We will separately consider \textsf{VAPE} and \textsf{VOPE} coupling and realize that the message passing for \textsf{VAPE} coupling is almost equivalent to the proposed PC architecture, while the \textsf{VOPE} coupling comes with more challenges, simply because it hasn't been addressed in detail in existing PC proposals on neural architecture.

\loadfigure[fig:vaall]{figures/vape_all}

\subsection{VAPE coupling}

Figure \ref{fig:vaall} sketches a possible architecture implementing all computations involved in \textsf{VAPE} coupling for three example cortical regions (levels). The assignment of neural elements to cortical layers follows the proposal in in figure 3 of \cite{Shipp2016}. For example, we've placed all precision-related nodes into the upper layers, while expectations and predictions of the mean live in intermediate and deep layers, respectively. In the following, we go through the different computational steps and note differences to the proposal for predictive coding.

For the \textsf{PE} step, we have the following computation:

\begin{equation}
	\delta_i^{(k)} = \mu_i^{(k)} - \hat{\mu}_i^{(k)}.
\end{equation}

\loadfigure[fig:vape]{figures/vape_pe}

The message passing implicated is depicted in figure \ref{fig:vape} and is in line with PE computations as suggested by \cite{Shipp2016}.

In the \textsf{Update} step, the posterior estimates of mean and variance are given by:

\begin{equation}
	\mu_i^{(k)} = \hat{\mu}_i^{(k)} + \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)}
\end{equation}

and

\begin{equation}
	\pi_i^{(k)} = \hat{\pi}_i^{(k)} + \alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}.
\end{equation}

A possible message passing implementation for these updates is shown in figure \ref{fig:vaupdate}. There are two important differences to \cite{Shipp2016} to note here.\\

\loadfigure[fig:vaupdate]{figures/vape_update}

First, in \textsf{PC} proposal, the update of the mean $\mu_i$ is driven both by the lower-level PE $\delta_{i-1}$ and the PE on the level itself, $\delta_i$, which mediates the influence of the empirical prior, or the prediction, on the mean. 

In the HGF equations, we do not have this negative feedback loop from $\delta_i$ to $\mu_i$. Instead, the prediction $\hat{\mu}_i$ has a direct influence on $\mu_i$ (see equations above), hence the arrow from the $\hat{\mu}_i$ node to the $\mu_i$ node in figure \ref{fig:vaupdate}. 

However, by considering the within-trial temporal evolution of the $\mu_i$ node, we can get the same feedback loop. Noting that $\mu_i$ as computed in the above equation will be the endpoint of the update, i.e., the equilibrium value to which the node should stabilize at the end of trial $k$, we can, in the style of \cite{Bogacz2017}, propose the following temporal evolution:

\begin{equation}
	\dot{\mu}_i^{(k)} = \hat{\mu}_i^{(k)} + \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \mu_i^{(k)}
\end{equation}

Clearly, a node with this dynamic converges to the required posterior value, which can be seen by setting $\dot{\mu}_i$ to zero. Now we note that

\begin{equation}
	\dot{\mu}_i^{(k)} = \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \mu_i^{(k)} + \hat{\mu}_i^{(k)}
\end{equation}

can be summarized as 

\begin{equation}
	\dot{\mu}_i^{(k)} = \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \delta_i^{(k)}.
\end{equation}

Following this logic, we could also place the arrow between $\delta_i$ and $\mu_i$. The exercise of writing down the within-trial temporal dynamics of the nodes will be repeated below for all nodes, which results in equations for within-trial network dynamics. These can be used to simulate within-trial responses of all nodes, examining network stability, and predicting neural dynamics which could for example be compared with electrophysiological data. 

The second difference to the \textsf{PC} proposal is rather irreducible. As already mentioned in the previous section, the messages sent up to parent nodes do not only consist of prediction errors or their precision weights. Instead, to update the precision node $\pi_i$, node $i$ also needs access to the prediction precision $\hat{\pi}_{i-1}$ from the level below. This is reflected in the violet arrow travelling from superficial layers of level $i$ to level $i+1$. 

Finally, in the \textsf{Prediction} step, we have:

\begin{equation}
	\hat{\mu}_i^{(k+1)} = \mu_i^{(k)} + \alpha_{i,i+1} \mu_{i+1}^{(k)}
\end{equation}

and

\begin{equation}
	\hat{\pi}_i^{(k+1)} = \frac{1}{\frac{1}{\pi_i^{(k)}} + \exp(\omega_i)}.
\end{equation}

\loadfigure[fig:vaprediction]{figures/vape_prediction}

This again seems unproblematic, although the arrow from $\mu_i$ to $\hat{\mu}_i$ would not appear in the classical \textsf{PC} architecture. Also, we've placed the predicted precision $\hat{\pi}$ in the superficial layers here, for convenience, as it interacts only with the posterior precision $\pi$ and acts as a weight on the prediction error, and also inaccordance with \cite{Shipp2016}, where precision signals arise from layers 2 and 3A. 

\subsection{VOPE coupling}
For volatility parents, we propose the following: Each level of a cortical hierarchy implements its own volatility parent in superficial layers. Coupling across layers is exclusively value-related. This means that having a volatility parent is nothing more but a more sophisticated way of predicting and updating a level's precision estimates. \\

This setup excludes volatility parents of volatility parents. However, it allows for a cortical hierarchy where higher levels predict either the mean, or the volatility, of lower levels, both via value coupling. In other words, any given level receives top-down predictions not only about its mean, but also about its volatility, which is implemented as the mean of its volatility parent.\\

Note that this setup also allows for a centralized or global volatility estimation. For example, a cortical or subcortical region which monitors the overall environmental volatility could be coupled to all local volatility units via value coupling. Our proposal is more flexible than previous ones \cite{Kanai2015} though, in that it also enables differential volatility estimation, e.g. within different sensory channels. \\

Moreover, in principle, both the mean $\mu_i$ as well as the volatility estimate $\check{\mu}_i$ of a given level could have (one or more) value parents in higher cortical areas. This allows for both separate and combined higher-level prediction of both the precision and the mean of level $i$.\\

Due to our proposal, we slightly change the notation for volatility coupling from here on. Level $i$ from now on refers to hierarchical level within a cortical hierarchy. Nodes on this level comprise the 'original' nodes $\mu_i$, $\pi_i$, $\hat{\mu}_i$, $\hat{\pi}_i$, and $\delta_i$, which are concerned with value estimation, but also all the nodes belonging to a volatility parent of these nodes, which we now denote as $\check{\mu}_i$, $\check{\pi}_i$, $\hat{\check{\mu}}_i$, $\hat{\check{\pi}}_i$, and $\check{\delta}_i$. Here, $\check{\delta}_i$ refers to the value prediction error about the mean of the volatility, $\check{\mu}_i$. \\

The \textsf{VOPE} $\Delta_i$, which we have written as a function of the \textsf{VAPE} $\delta_i$, of level $i$, is what communicates between a level's nodes and a level's volatility parent's nodes. Value estimation and volatility estimation both happen within on cortical level or column. For the ensuing equations (simple reformulations of the computations in \textsf{VOPE} coupling as presented before) and visualizations of this proposal, please refer to the appendix. \\

Discuss similiraties with and differences to \cite{Kanai2015} here!\\


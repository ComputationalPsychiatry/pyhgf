\section{Within-trial time dynamics}

In this section, we derive differential equations which lay out the within-trial update dynamics entailed by the HGF. To this end, we consider the posterior values of all our nodes (quantities) as given by the HGF update equations as the equilibrium point towards which all dynamics must converge. \\

We will note that these equations imply a slightly different coupling between nodes compared to the previous section. On first glance, this complicates the picture, as we're introducing more and more nodes. On the other hand, we increase biological plausibility by making the computations of a single node or connection simpler and simpler.\\

We divide the equations into \textsf{value} estimation and \textsf{volatility} estimation, noting that \textsf{VAPE} and \textsf{VOPE} coupling are not useful divisions anymore (the mean of the value estimation of a level $i$, $\mu_i$, will be \textsf{VAPE}-coupled to its parent $\mu_{i+1}$, while the precision $\pi_i$ of the value estimation will be \textsf{VOPE}-coupled to the volatility parent $\check{\mu}_i$ on level $i$ itself; on the other hand, the mean of the volatility estimation, $\check{\mu}_i$, can again have a \textsf{VAPE} coupling to a higher level, e.g. $\mu_{i+2}$, and the precision of volatility, $\check{\pi}_i$, will, in the current framework, not be coupled to any parent, i.e., its prediction will depend only on the parameter $\check{\omega_i}$).

\subsection{Value estimation}

For the \textsf{PE} about the mean, we want to reach the following posterior:

\begin{equation}
	\delta_i^{(k)} = \mu_i^{(k)} - \hat{\mu}_i^{(k)}.
\end{equation}

This can be easily achieved by a node with these dynamics:

\begin{equation}
	\dot{\delta}_i^{(k)} = \mu_i^{(k)} - \hat{\mu}_i^{(k)} - \delta_i^{(k)}.
\end{equation}

From this it follows that we require an additional inhibitory self-connection for the \textsf{PE} node.\\

We now consider an alternative implementation for the \textsf{PE} computation, which will turn out to be very useful.\\

Instead of considering the unweighted $\delta_i$, we can also directly model a node which corresponds to the precision-weighted prediction error $\varepsilon_i$:

\begin{equation}
	\varepsilon_i = \frac{\alpha^2 \hat{\pi}_i}{\pi_{i+1}} (\mu_i - \hat{\mu}_i) = \frac{\alpha^2 \hat{\pi}_i}{\pi_{i+1}} \delta_i.
\end{equation}

This node could evolve according to
\begin{equation}
	\dot{\varepsilon}_i = \mu_i - \hat{\mu}_i -  \frac{\pi_{i+1}}{\alpha^2 \hat{\pi}_i} \varepsilon_i,
\end{equation}

which allows us to re-introduce the unweighted \textsf{PE} $\delta_i$:

\begin{equation}
	\begin{split}
		\delta_i &= \frac{\pi_{i+1}}{\alpha^2 \hat{\pi}_i} \varepsilon_i \\
		&= \frac{\pi_{i+1} (\mu_i - \hat{\mu}_i) \alpha^2 \hat{\pi}_i}{\alpha^2 \hat{\pi}_i \pi_{i+1}}\\
		&= \mu_i - \hat{\mu}_i,\\
	\end{split}
\end{equation}

such that

\begin{equation}
	\dot{\varepsilon}_i = \mu_i - \hat{\mu}_i - \delta_i
\end{equation}

and

\begin{equation}
	\dot{\delta}_i = \varepsilon_i - \alpha^2 \frac{\hat{\pi}_i}{\pi_{i+1}} \delta_i.
\end{equation}

This has the advantage that the weighted \textsf{PE} $\varepsilon_i$ can travel up the hierarchy to update higher levels, while the unweighted \textsf{PE} $\delta_i$ can be used to feedback to the mean on the same level, an element that will be introduced next.\\ 

In the \textsf{Update} step, the posterior estimate of the mean is given by:

\begin{equation}
	\mu_i^{(k)} = \hat{\mu}_i^{(k)} + \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)}.
\end{equation}

Here, we propose the following temporal evolution:

\begin{equation}
	\dot{\mu}_i^{(k)} = \hat{\mu}_i^{(k)} + \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \mu_i^{(k)}
\end{equation}

Clearly, a node with this dynamic converges to the required posterior value, which can be seen by setting $\dot{\mu}_i$ to zero. Now we note that

\begin{equation}
	\dot{\mu}_i^{(k)} = \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \mu_i^{(k)} + \hat{\mu}_i^{(k)}
\end{equation}

can be summarized as 

\begin{equation}
	\dot{\mu}_i^{(k)} = \varepsilon_{i-1}^{(k)} - \delta_i^{(k)}.
\end{equation}

We will therefore consider a connection between $\delta_i$ and $\mu_i$, instead of connecting the prediction node $\hat{\mu}_i$ with the mean $\mu_i$. As already outlined in the previous section, this also resolves one of the apparent differences between our message passing scheme and classical PC proposals.\\

For the posterior precision, we have

\begin{equation}
	\pi_i^{(k)} = \hat{\pi}_i^{(k)} + \alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}.
\end{equation}

Here, we stick with the same approach as for the \textsf{PE} node, and simply add a self-inhibitory connection to implement the dynamics:

\begin{equation}
	\dot{\pi}_i^{(k)} = \hat{\pi}_i^{(k)} + \alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)} - \pi_i^{(k)}.
\end{equation}

Finally, in the \textsf{Prediction} step, we want to reach the following prediction of the mean:

\begin{equation}
	\hat{\mu}_i^{(k+1)} = \mu_i^{(k)} + \alpha_{i,i+1} \mu_{i+1}^{(k)}.
\end{equation}

Rewriting this as a differential equation

\begin{equation}
	\dot{\hat{\mu}}_i^{(k+1)} = \alpha_{i,i+1} \mu_{i+1}^{(k)} + \mu_i^{(k)} - \hat{\mu}_i^{(k+1)}, 
\end{equation}

we again see the \textsf{PE} node appear:

\begin{equation}
	\dot{\hat{\mu}}_i^{(k+1)} = \alpha_{i,i+1} \mu_{i+1}^{(k)} + \delta_i^{(k)}. 
\end{equation}

Note that $\delta_i^{(k)}$ is actually defined in terms of $\hat{\mu}_i^{(k)}$, and not $\hat{\mu}_i^{(k+1)}$, but given that we are in continuous time now, we suspect that this is probably equivalent here or can be resolved by introducing a delay between the computation of the prediction and the prediction error. For simplicity, we will skip the trial indices from now on. \\

The precision of this prediction should converge to:

\begin{equation}
	\hat{\pi}_i = \frac{1}{\frac{1}{\pi_i} + \exp(\omega_i)}.
\end{equation}

We can write the dynamics as:

\begin{equation}
	\begin{split}
		\dot{\hat{\pi}}_i &= 1 - (\frac{1}{\pi_i} + \exp(\omega_i)) \hat{\pi}_i\\
		&= 1 - \frac{\hat{\pi}_i}{\pi_i} - \exp(\omega_i) \hat{\pi}_i.\\
	\end{split}
\end{equation}

To simplify the implementation, we introduce an additional node $p_{i1}$:

\begin{equation}
	p_{i1} = \frac{\hat{\pi}_i}{\pi_i}.
\end{equation}

This new node can have the following dynamcis:
\begin{equation}
	\dot{p}_{i1} = \hat{\pi}_i - \pi_i p_{i1}, 
\end{equation}

and the dynamics for the precision of the prediction $\hat{\pi}_i$ become

\begin{equation}
	\dot{\hat{\pi}}_i = 1 - p_{i1} - \exp(\omega_i) \hat{\pi}_i,
\end{equation}

where the last term can be implemented as a self-inhibition with a connection weight of $\exp(\omega_i)$. However, to facilitate the implementation of the influence of a volatility parent on the precision estimate, we introduce another additional node, $p_{i2}$, here and also already consider a node that estimates volatility, $\nu_i$. In the case of value estimation without a volatility node, this estimate will only depend on the parameter value $\omega_i$, i.e., the tonic volatility.

The dynamics of $\hat{\pi}_i$ then become:
\begin{equation}
	\dot{\hat{\pi}}_i = 1 - p_{i1} - p_{i2},
\end{equation}

with the node $p_{i2}$ defined as 

\begin{equation}
	p_{i2} = \hat{\pi}_i \exp(\omega_i),
\end{equation}

with dynamics described by

\begin{equation}
	\dot{p}_{i2} = \exp(\omega_i) \hat{\pi}_i - p_{i2}
\end{equation}

in the absence of a volatility parent (see below for the extension to volatility estimation). We introduce the volatility estimate $\nu_i$ as

\begin{equation}
	\nu_i = \exp(\omega_i),
\end{equation}
 
 evolving as

\begin{equation}
	\dot{\nu}_i = \exp(\omega_i) - \nu_i,
\end{equation}

leading to 

\begin{equation}
	\dot{p}_{i2} = \nu_i \hat{\pi}_i - p_{i2}.
\end{equation}

\loadfigure[fig:vapediffnonu]{figures/diffeq_vape_without_nu}

A visualization of the network entailed by the differential equations so far - in the absence of volatility estimation - is depicted in figure \ref{fig:vapediffnonu}.

Note that the dynamics of $\nu_i$ can be implemented by a self-inhibition and a positive influence from a tonicly active neuron (with activity 1) with a connection weight of $\exp(\omega_i)$. The advantage of this description is that if we add a volatility parent, we simply have to let this tonicly active neuron be driven by the volatility estimate $\check{\mu}_i$, weighted by the coupling parameter $\kappa_i$, to arrive at 

\begin{equation}
	\nu_i = \exp(\kappa_i \check{\mu}_i) \exp(\omega_i).
\end{equation}

Here we note that according to our proposal above, any cortical level $i$ will implement its own volatility estimation, along with the value estimation. Therefore, the precision of the prediction, $\hat{\pi}_i$, will always depend on the value of the volatility parent $\check{\mu}_i$:

\begin{equation}
	\hat{\pi}_i = \frac{1}{\frac{1}{\pi_i} + \exp(\kappa_i \check{\mu}_i + \omega_i)}.
\end{equation}

\loadfigure[fig:vapediff]{figures/diffeq_vape}

This variant of the \textsf{VAPE} implementation, which is representing volatility estimation explicitely even in the absence of a \textsf{VOPE} parent, is depicted in figure \ref{fig:vapediff}.

We now turn to the computations entailed by phasic volatility estimation, including the modified dynamics for $\hat{\pi}_i$ and $\nu_i$. 

\subsection{Volatility estimation}

Here we start by considering the changes in the dynamics of the precision prediction in node $i$, i.e. the dynamics of $\hat{\pi}_i$ and $\nu_i$, which are the consequence of adding an estimate of phasic volatility $\check{\mu}_i$.

We simply have to add a (multiplicative) term in the dynamics of the predicted volatility $\nu_i$, which will, through the dependence of $p_{i2}$ on $\nu_i$ and $\hat{\pi}_i$ on $p_{i2}$, lead to the desired influence:

\begin{equation}
	\dot{\nu}_i = \exp(\kappa_i \check{\mu}_i) \exp(\omega_i) - \nu_i.
\end{equation}

Additionally, to simplify the precision weighting of the volatility prediction error, we use the auxiliary expected precision node $\gamma_i$ as introduced in the previous section:

\begin{equation}
  \gamma_i = \kappa_i \nu_i \hat{\pi}_i
\end{equation}

with, for example, 

\begin{equation}
  \dot{\gamma}_i = \kappa_i \nu_i \hat{\pi}_i - \gamma_i.
\end{equation}

For the \textsf{PE} about the level of volatility $\Delta_i$, we have previously seen that we want to reach the following posterior:

\begin{equation}
	\Delta_i^{(k)} = \frac{\hat{\pi}_i^{(k)}}{\pi_{i}^{(k)}} + \hat{\pi}_i^{(k)} (\delta_i^{(k)})^2 - 1.
\end{equation}

This corresponds to the unweighted volatility prediction error. Again, we start by considering a node which in turn reflects the precision-weighted (VO)PE, which we will call $E_i$:

\begin{equation}
	E_i^{(k)} = \frac{1}{2} \frac{\gamma_i^{(k)}}{\check{\pi}_i^{(k)}} \Delta_i^{(k)}.
\end{equation}

This could evolve according to 

\begin{equation}
	\dot{E}_i^{(k)} = \Delta_i^{(k)} - \frac{2 \check{\pi}_i^{(k)}}{\gamma_i^{(k)}} E_i^{(k)},
\end{equation}

which can also be written as 

\begin{equation}
	\dot{E}_i^{(k)} = \frac{\hat{\pi}_i^{(k)}}{\pi_{i}^{(k)}} + \hat{\pi}_i^{(k)} (\delta_i^{(k)})^2 - 1 - \Delta_i^{(k)},
\end{equation}

with the unweighted PE node $\Delta_i$

\begin{equation}
	\Delta_i^{(k)} = \frac{2 \check{\pi}_i^{(k)}}{\gamma_i^{(k)}} E_i^{(k)}
\end{equation}

evolving as 

\begin{equation}
	\dot{\Delta}_i^{(k)} = E_i^{(k)} - \frac{\gamma_i^{(k)}}{2 \check{\pi}_i^{(k)}} \Delta_i^{(k)}.
\end{equation}

\loadfigure[fig:vopediff]{figures/diffeq_vope}

This leads us to the same setup as for the value prediction error nodes, where we considered both an $\varepsilon_i$ node and an unweighted PE node $\delta_i$, as depicted in figure \ref{fig:vopediff}. In the case of volatility estimation, this again turns out to be useful: As before, the precision-weighted PE node $E_i$ can be passed on to \textsf{Update} the volatility parent node $\check{\mu}_i$:

\begin{equation}
	\check{\mu}_i^{(k)} = \hat{\check{\mu}}_i^{(k)} + \frac{1}{2} \frac{\gamma_i^{(k)}}{\check{\pi}_i^{(k)}} \Delta_i^{(k)} = \hat{\check{\mu}}_i^{(k)} + E_i^{(k)}
\end{equation}

with 

\begin{equation}
	\dot{\check{\mu}}_i^{(k)} = E_i^{(k)} - (\check{\mu}_i^{(k)} - \hat{\check{\mu}}_i^{(k)}) = E_i^{(k)} - \check{\delta}_i.
\end{equation}

There are three things worth mentioning here. First, in contrast to the previous section, we now use $\gamma_i$ and $\Delta_i$ in the update equations (instead of $\gamma_{i-1}$ and $\Delta_{i-1}$). This is simply due to the fact that we have put the volatility parent $\check{\mu}_i$ into the same cortical level $i$ as the node $\mu_i$ itself. Instead of moving from level $i$ to level $i+1$ for the volatility estimation, we now denote value-related nodes as $\mu$ and volatility-related nodes as $\check{\mu}$.

Second, the (unweighted) PE node $\check{\delta_i}$, which is used to feedback to the update of $\check{\mu}_i$, is again a value PE, but now about the mean of the volatility estimate. This \textsf{VAPE} can again be used (in its precision-weighted form $\check{\varepsilon}_i$) to communicate with higher cortical levels $\mu_{i+1}$, completely parallel to the precision-weighted value prediction about the mean, $\varepsilon_i$.

Third, we note that in volatility estimation, we have to distinguish between the (weighted and unweighted) prediciton errors that are used to \textit{update} the volatility estimate, $E$ and $\Delta$, and the (weighted and unweighted) prediciton errors \textit{about} the volatility estimate, $\check{\varepsilon}_i$ and $\check{\delta}_i$, which will be used to update any higher cortical level $i+1$ predicting this volatility estimate. 

Finally, although the unweighted volatility PE $\Delta_i$ is not needed for a feedback to $\mu_i$ or $\check{\mu}$, it is still very useful as it is used in the update of the precision of the volatility estimation $\check{\pi}_i$:

\begin{equation}
	\check{\pi}_i^{(k)} = \hat{\check{\pi}}_i^{(k)} + \frac{1}{2} (\gamma_i^{(k)})^2 + (\gamma_i^{(k)})^2 \Delta_i^{(k)} - \frac{1}{2} \gamma_i^{(k)} \Delta_i^{(k)}, 
\end{equation}

which can be summarized as 

\begin{equation}
	\check{\pi}_i^{(k)} = \hat{\check{\pi}}_i^{(k)} + \frac{1}{2} (\gamma_i^{(k)})^2 + ((\gamma_i^{(k)})^2 - \frac{1}{2} \gamma_i^{(k)}) \Delta_i^{(k)}.
\end{equation}

This could have the temporal evolution

\begin{equation}
	\dot{\check{\pi}}_i^{(k)} = \hat{\check{\pi}}_i^{(k)} + \frac{1}{2} (\gamma_i^{(k)})^2 + ((\gamma_i^{(k)})^2 - \frac{1}{2} \gamma_i^{(k)}) \Delta_i^{(k)} - \check{\pi}_i^{(k)}.
\end{equation}

Finally, turning towards the \textsf{Prediction} step in the volatility estimation, this is again very simple, as we only consider value parents for volatility nodes. For the prediction of the mean, we have

\begin{equation}
	\hat{\check{\mu}}_i^{(k+1)} = \check{\mu}_i^{(k)} + \alpha_{i,i+1} \mu_{i+1}^{(k)}, 
\end{equation}

which could evolve as 

\begin{equation}
	\dot{\hat{\check{\mu}}}_i^{(k+1)} = \check{\mu}_i^{(k)} - \hat{\check{\mu}}_i^{(k+1)} + \alpha_{i,i+1} \mu_{i+1}^{(k)} = \check{\delta}_i + \alpha_{i,i+1} \mu_{i+1}^{(k)}, 
\end{equation}

and for the precision of that prediction, we get

\begin{equation}
  \hat{\check{\pi}}_i^{(k+1)} = \frac{1}{\frac{1}{\check{\pi}_i^{(k)}} + \exp(\check{\omega}_i)}, 
\end{equation}

which could evolve as shown above in the value estimation (using the auxiliary nodes $\check{p}_{i1}$):

\begin{equation}
	\dot{\hat{\check{\pi}}}_i = 1 - \check{p}_{i1} - \hat{\check{\pi}}_i \exp(\check{\omega}_i)
\end{equation}

with

\begin{equation}
	\dot{\check{p}}_{i1} = \hat{\check{\pi}}_i - \check{\pi}_i \check{p}_{i1}. 
\end{equation}

Note that we do not bother to introduce $\check{p}_{i2}$ or $\check{\nu}_i$ here, as we exclude the possibility of a volatility parent $\check{\mu}_i$ having a volatility parent.
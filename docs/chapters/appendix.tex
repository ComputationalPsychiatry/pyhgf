\section{Appendix}

\subsection{Computations in VOPE coupling}

In the following, we present the computations of volatility nodes according to the proposal that each cortical level of a hierarchy implements its own volatility estimation.\\

The \textsf{Update} steps in volatility coupling are:

\begin{equation}
	\check{\mu}_i^{(k)} = \hat{\check{\mu}}_i^{(k)} + \frac{1}{2} \frac{\kappa_i \gamma_{i}^{(k)}}{\check{\pi}_i^{(k)}} \Delta_{i}^{(k)}
\end{equation}

\begin{equation}
	\check{\pi}_i^{(k)} = \hat{\check{\pi}}_i^{(k)} + \frac{1}{2} (\kappa_i \gamma_{i}^{(k)})^2 + (\kappa_i \gamma_{i}^{(k)})^2 \Delta_{i}^{(k)} - \frac{1}{2} \kappa_i^2 \gamma_{i}^{(k)} \Delta_{i}^{(k)}
\end{equation}

In the \textsf{PE} step, the node computes:

\begin{equation}
  \Delta_i^{(k)} = \frac{\hat{\pi}_i^{(k)}}{\pi_{i}^{(k)}} + \hat{\pi}_i^{(k)} (\delta_i^{(k)})^2 - 1. 
\end{equation}

Finally, In the \textsf{Prediction} step, we now need to compute four nodes: \\
the predicted (volatility) mean

\begin{equation}
	\hat{\check{\mu}}_i^{(k+1)} = \check{\mu}_i^{(k)},
\end{equation}

the precision of that prediction

\begin{equation}
  \hat{\check{\pi}}_i^{(k+1)} = \frac{1}{\frac{1}{\check{\pi}_i^{(k)}} + \nu_i^{(k+1)}}, 
\end{equation}

the predicted environmental uncertainty (as a function of the next higher level in the hierarchy, $\mu_{i+1}$)

\begin{equation}
  \nu_i^{(k+1)} = \exp(\kappa_{i,i+1} \mu_{i+1}^{(k)} + \omega_i),
\end{equation}

and the new (auxiliary) expected precision

\begin{equation}
  \gamma_i^{(k+1)} = \nu_i^{(k+1)} \hat{\pi}_i^{(k+1)}.
\end{equation}

The last node is only defined for convenience in terms of simplifying the equations and the corresponding message passing.


\subsection{Possible PC-like implementation of VOPE coupling}

\loadfigure[fig:voall]{figures/vope_all}
\loadfigure[fig:withvol]{figures/zoom_vape_with_vope}
\loadfigure[fig:volparent]{figures/zoom_vope}

Figure \ref{fig:voall} displays one proposal for within-column volatility estimation, where we've zoomed in to a level $i$ of the cortical hierarchy (and a value parent $i+1$) and added its volatility parent to the superficial layers. \\

An alternative implementation of this idea is displayed in figure \ref{fig:withvol}. Here, only the precision-related computations of the volatility parent are placed in the superficial layers, while the corresponding prediction errors, $\Delta_i$ and $\check{\delta}_i$ as well as the actual volatility estimate $\check{\mu}_i$ live in middle layers, and the prediction of the volatility estimate, $\hat{\check{\mu}}_i$, lives in layer 6. This setup stresses the structural similarities to the message passing entailed by mean coupling. Moreover, using this setup, we can now easily depict how a higher cortical level $i+1$ would serve as a value parent to the volatility estimate in level $i$ - instead of predicting the mean - by simply exchanging the arrows between the levels as shown in figure \ref{fig:volparent}. 


\subsection{Computations of input nodes}
The main steps that need to happen within an input node on a given trial are:
\begin{itemize}
	\item receive a new input and store it
	\item either receive as a second input the exact time of the input, or infer the time as 'plus 1' (next trial)
	\item compute prediction errors and whatever else needs to be signalled bottom-up to the first actual HGF node
	\item send bottom-up: usually input or PE, some estimate of precision, and time
	\item receive top-down: usually $\hat{\mu}$ from the parent
	\item compute surprise, given input and prediction
\end{itemize}
The quantities being signalled bottom-up, and the computation of surprise, depend on the nature of the input node (continuous or binary) and on the nature of the coupling with the parent.

Because the input nodes are not full HGF nodes, but rather serve as a relay station for the input and for computing surprise, the message passing (and the within-node computations) differs from the previously discussed scheme.

\subsubsection{Continuous input nodes}
A continuous input node can compute two prediction errors: \textsf{VAPE}s and \textsf{VOPE}s. We will first deal with the computations related to \textsf{VAPE} coupling of the continuous input node with a parent HGF node. Note that an input node will always have a \textsf{VAPE} or value parent, while having a volatility parent is optional.\\

\textbf{Value parents of continuous input nodes}\\
In the \textsf{PE step}, the \textsf{VAPE} will be computed as the difference the prediction and the posterior, where the current prediction $\hat{\mu}_{vapa}$ of the value parent $vapa$ is used as the prediction, and the value of the input itself, $u$, is used as the posterior:

\begin{equation}
	\delta_{inp}^{(k)} = \mu_{inp}^{(k)} - \hat{\mu}_{inp}^{(k)} = u^{(k)} - \hat{\mu}_{vapa}^{(k)}
\end{equation}

This means that prior to the update of the input node, it needs to receive the current prediction $\hat{\mu}_{vapa}^{(k)}$ of its parent node. \\

If one wanted to construct an \textsf{UPDATE step} for the input node, the posterior mean simply amounts to the input itself, while the posterior precision will be determined by the value parent's precision:
\begin{align}
	\mu_{inp}^{(k)} &= u^{(k)}\\
	\pi_{inp}^{(k)} &= \pi_{vapa}^{(k)}
\end{align}

The update of the value parent node will look like the regular \textsf{VAPE} updates from previous chapters:
\begin{align}
	\pi_{vapa}^{(k)} &= \hat{\pi}_{vapa}^{(k)} + \hat{\pi}_{inp}^{(k)}\\
	\mu_{vapa}^{(k)} &= \hat{\mu}_{vapa}^{(k)} + \frac{\hat{\pi}_{inp}^{(k)}}{\pi_{vapa}^{(k)}} \delta_{inp}^{(k)}
\end{align}

That means that the input node needs to signal bottom-up to its value parent:
\begin{description}
	\item[Predicted precision:] 		$\hat{\pi}_{inp}^{(k)}$
	\item[Prediction error:]			$\delta_{inp}^{(k)}$.
\end{description}

Note that the connection between a continuous input node and its value parent would always have a connection weight of $\alpha = 1$.\\

In the \textsf{PREDICTION step}, the input node would have to compute the predicted mean $\hat{\mu}_{inp}^{(k+1)}$ and the predicted precision $\hat{\pi}_{inp}^{(k+1)}$ for the next trial. However, as described above, the predicted mean will be received from the value parent: 
\begin{equation}
	\hat{\mu}_{inp}^{(k+1)} = \hat{\mu}_{vapa}^{(k+1)}
\end{equation}
Given that the value parent might operate under a drift parameter, the current prediction can only be computed given the time of the next input, i.e., at the beginning of the next trial, when the new input (and its time information) comes in. Then it needs to be signalled top-down immediately.\\

The predicted precision of the input node is a function of the input node's $\omega$ parameter, i.e., a constant, in the absence of a volatility parent:
\begin{equation}
	\hat{\pi}_{inp}^{(k+1)} = \frac{1}{\exp(\omega_{inp})}
\end{equation}

However, in the presence of a volatility parent, this will additionally depend on the posterior $\mu_{vopa}^{(k)}$ of that parent and the coupling parameter $\kappa_{vopa,inp}$ of the input node with its volatility parent $vopa$:
\begin{equation}
	\hat{\pi}_{inp}^{(k+1)} = \frac{1}{\exp(\kappa_{vopa,inp} \mu_{vopa}^{(k)} + \omega_{inp})}.
\end{equation}

Finally, to compute the surprise associated with the current input, the node needs to compute the negative log of the probability of input $u^{(k)}$ under a Gaussian prediction with $\hat{\mu}_{vapa}^{(k)}$ as mean and $\hat{\pi}_{inp}^{(k)}$ as the precision:
\begin{equation}
	-\log(p(u^{(k)})) = \frac{1}{2} (\log(2\pi) - \log(\hat{\pi}_{inp}^{(k)}) + \hat{\pi}_{inp}^{(k)} (u^{(k)} - \hat{\mu}_{vapa}^{(k)})^2).
\end{equation}

\textbf{Volatility parents of continuous input nodes}\\
Having a volatility parent for a continuous input node means that a \textsf{VOPE} will be computed and signalled bottom-up during the \textsf{PE step}. Importantly, the definition of the \textsf{VOPE} differs from previous definitions in that both the posterior precision as well as the posterior mean are taken from the value parent $vapa$:

\begin{equation}
  \Delta_{inp}^{(k)} = \frac{\hat{\pi}_{inp}^{(k)}}{\pi_{vapa}^{(k)}} + \hat{\pi}_i^{(k)} (u^{(k)} - \mu_{vapa}^{(k)})^2 - 1. 
\end{equation}

This means that the \textsf{VOPE} is not a direct function of the \textsf{VAPE} anymore, which was instead defined in terms of the difference between the input $u^{(k)}$ (as the posterior) and the predicted mean of the value parent $\hat{\mu}_{vapa}^{(k)}$ as the prediction. This in turn requires that the update of the value parents happens before the computation of the \textsf{VOPE}, and the posterior of the value parent is already available to the input node.\\

The \textsf{UPDATE step} for the volatility parent is also unusual in that the expected precision term $\gamma_{inp}$ is fixed to 1. Hence the update of the mean reads:
\begin{align}
	\mu_{vopa}^{(k)} &= \hat{\mu}_{vopa}^{(k)} 
			+ \frac{1}{2} \frac{\kappa_{vopa,inp} \gamma_{inp}^{(k)}}{\pi_{vopa}^{(k)}} \Delta_{inp}^{(k)}\\
			&= \hat{\mu}_{vopa}^{(k)} 
			+ \frac{1}{2} \frac{\kappa_{vopa,inp}}{\pi_{vopa}^{(k)}} \Delta_{inp}^{(k)}.
\end{align}

This simply means that along with the modified \textsf{VOPE}, the input node needs to signal a value of $1$ as the expected precision on every trial. Setting $\gamma_{inp}$ to $1$ in the update of the precision of the volatility parent leads to:

\begin{align}
	\pi_{vopa}^{(k)} &= \hat{\pi}_{vopa}^{(k)} 
			+ \frac{1}{2} (\kappa_{vopa,inp} \gamma_{inp}^{(k)})^2 
			+ (\kappa_{vopa,inp} \gamma_{inp}^{(k)})^2 \Delta_{inp}^{(k)} 
			- \frac{1}{2} \kappa_{vopa,inp}^2 \gamma_{inp}^{(k)} \Delta_{inp}^{(k)}\\
			&= \hat{\pi}_{vopa}^{(k)} 
			+ \frac{1}{2} (\kappa_{vopa,inp})^2 
			+ (\kappa_{vopa,inp})^2 \Delta_{i-1}^{(k)} 
			- \frac{1}{2} \kappa_{vopa,inp}^2 \Delta_{i-1}^{(k)}\\
			&= \hat{\pi}_{vopa}^{(k)} 
			+ \frac{1}{2} (\kappa_{vopa,inp})^2 
			+ \frac{1}{2} (\kappa_{vopa,inp})^2 \Delta_{i-1}^{(k)}\\
			&= \hat{\pi}_{vopa}^{(k)} 
			+ \frac{1}{2} (\kappa_{vopa,inp})^2 (1 + \Delta_{i-1}^{(k)}).
\end{align}

Finally, the \textsf{PREDICTION step} of a continuous input node will be modified by the presence of a volatility parent in that the predicted precision now also depends on the posterior mean of the volatility parent:

\begin{equation}
	\hat{\pi}_{inp}^{(k+1)} = \frac{1}{\exp(\kappa_{inp} \mu_{vopa}^{(k)} + \omega_{inp})}.
\end{equation}

\textbf{Peculiarities of continuous input nodes and consequences for their parents}\\
First, due to the dependence of the \textsf{VOPE} on the posterior beliefs of the value parent, the continuous input node needs to communicate with its value parent first and wait for the posteriors to be computed there and sent top-down in order to elicit a new update in its volatility parent.\\

Second, the value parent needs to send top-down not only the posterior mean, but also the posterior precision, for the same reason.\\

Third, the connection weight for value connections will always be $\alpha = 1$.\\

Fourth, for issuing a new prediction $\hat{\mu}_{inp}$, the node needs to receive the predicted mean of its value parent at the beginning of a new trial. This means it must be possible to elicit a new prediction in continuous HGF nodes without actually sending a prediction error, instead by only sending a new time point. The HGF node needs to react to this by sending top-down the new predicted mean, such that the input node can compute the \textsf{PE} and signal it back bottom-up for a new update.\\

Thus, the steps for a continuous input node are:
\begin{itemize}
    \item receive input $u$
    \item determine time of input
    \item send bottom-up to value parent: time of input (to elicit a prediction)
    \item receive top-down: predicted mean $\hat{\mu}_{vapa}$
    \item compute prediction $\hat{\mu}_{inp}$ and retrieve $\hat{\pi}_{inp}$
    \item compute surprise using $u$, $\hat{\mu}_{inp}$ and $\hat{\pi}_{inp}$
    \item compute \textsf{VAPE} using $u$ and $\hat{\mu}_{inp}$
    \item send bottom-up to value parent: \textsf{VAPE}, $\hat{\pi}_{inp}$, and time
    \item receive top-down: posteriors $\mu_{vapa}$ and $\pi_{vapa}$
    \item compute \textsf{VOPE} using $u$, $\hat{\pi}_{inp}$, $\mu_{vapa}$ and $\pi_{vapa}$
    \item send bottom-up to volatility parent: \textsf{VOPE}, $\gamma_{inp} = 1$, and time
    \item receive top-down: posterior $\mu_{vopa}$
    \item compute new prediction for precision $\hat{\pi}_{inp}^{(k+1)}$ using $\mu_{vopa}^{(k)}$,
\end{itemize}
\noindent
and the value parent of a continuous input node needs to 
\begin{enumerate}
    \item be able to elicit new predictions based on time input
    \item send new predictions top-down immediately in the case of a continuous input node child
    \item send down not only its posterior mean, but also the precision after each update.
\end{enumerate}

\subsubsection{Binary input nodes}
Binary input nodes only have value parents. These value parents in turn are binary HGF nodes, which are special cases of HGF nodes, which themselves only have value parents.\\

For binary input nodes, the precision of the input prediction $\hat{\pi}_{inp}$ is constant, i.e. we can treat it as a parameter. We distinguish two cases: Either the precision is infinite, i.e., $\hat{\pi}_{inp} = \inf$, or in other words, there is no sensory noise, or the precision has a finite value.\\

In general, the steps for a binary input node are:
\begin{itemize}
    \item receive input $u$
    \item determine time of input
    \item compute prediction errors, if necessary
    \item send bottom-up: $u$ or two prediction errors, input precision, and time
    \item receive top-down: prediction of parent $\hat{\mu}_{pa}$
    \item compute surprise based on message from parent.
\end{itemize}

The bottom-up messages and the surprise computation depend on the distinction between infinite and finite precision. We will now lay these out for the two cases.\\

\noindent
\textbf{Infinite precision}\\
\noindent
This case is very simple. If $\hat{\pi}_{inp}$ is infinite, then the bottom-messages are simply $\hat{\pi}_{inp}$ itself, and $u$. The surprise computation is also very simple:
\begin{equation}
    surprise^{(k)} = \left.
    \begin{cases}
        -\log(1-\hat{\mu}_{pa}^{(k)}), & \text{for } u^{(k)} = 1\\
        -\log(\hat{\mu}_{pa}^{(k)}), & \text{for } u^{(k)} = 0.
    \end{cases}
    \right.
\end{equation}


\noindent
\textbf{Finite precision}\\
\noindent
Here, the input $u$ is actually not binary, but a real number whose distribution is a mixture of Gaussians. If $x_1 = 1$, the probability of $u$ is normally distributed with constant precision $\hat{\pi}_{inp}$ around a constant value $\eta_0$, corresponding to the most likely sensation if $x_1 = 1$. If, however, $x_1 = 0$, the most likely sensation is $\eta_0$ with the probability of $u$ normally distributed with the same precision.\\

The messages to be sent bottom-up will in this case again be the precision $\hat{\pi}_{inp}$, along with two prediction errors, namely the deviations of the input $u$ from both possible values $\eta_a$ and $\eta_b$:
\begin{align}
    \delta_{inp, 1}^{(k)} &= u^{(k)} - \eta_1\\
    \delta_{inp, 0}^{(k)} &= u^{(k)} - \eta_0.
\end{align}

Additionally, as always, the nodes need to send up the information about the time of the input. \\

For suprise computation, the node depends again on receiving the prediction of the parent node $\hat{\mu}_{pa}^{(k)}$, and the two $\delta$ values:
\begin{equation}
    surprise^{(k)} = -\log(\hat{\mu}_{pa}^{(k)} * \mathcal{N}(u^{(k)}; \eta_1, \hat{\pi}_{inp}) + (1-\hat{\mu}_{pa}^{(k)}) * \mathcal{N}(u^{(k)}; \eta_0, \hat{\pi}_{inp}))
\end{equation}

The special cases that follow for the update of the parent node are restricted to binary HGF nodes, which therefore represent their own special case of HGF nodes. 

\subsubsection{Binary HGF nodes}
Binary nodes are parents of binary input nodes. Their cycle thus starts with receiving a bottom-up message from their child node, which, depending on the precision of the child binary input node, can be comprised of 3 ($\hat{\pi}_{inp}$, $u^{(k)}$, and the time of the input), or 4 quantities ($\hat{\pi}_{inp}$, $\delta_{inp,1}^{(k)}$, $\delta_{inp,0}^{(k)}$, and the time of the input). This message elicits the \textsf{UPDATE step}, where the two cases are reflected as follows:
\begin{align}
    \mu_i^{(k)} &= u^{(k)}\\
    \pi_i^{(k)} &= \hat{\pi}_{inp}
\end{align}
if 
\begin{equation}
    \hat{\pi}_{inp} = \inf,
\end{equation}
and
\begin{align}
    \mu_i^{(k)} &= \frac{ \hat{\mu}_i^{(k)} * \exp(-\frac{1}{2} \hat{\pi}_{inp} (\delta_{inp, 1})^{2}) }{ \hat{\mu}_i^{(k)} * \exp(-\frac{1}{2} \hat{\pi}_{inp} (\delta_{inp, 1})^{2}) + (1-\hat{\mu}_i^{(k)}) * \exp(-\frac{1}{2} \hat{\pi}_{inp} (\delta_{inp, 0})^{2})  } \\
    \pi_i^{(k)} &= \frac{1}{\hat{\mu}_i^{(k)} * (1-\hat{\mu}_i^{(k)})}
\end{align}
if 
\begin{equation}
    \hat{\pi}_{inp} \neq \inf.
\end{equation}

In the \textsf{PE step}, the binary node computes a \textsf{VAPE} for its parent node (which is always a value parent):
\begin{equation}
    \delta_i^{(k)} = \mu_i^{(k)} - \hat{\mu}_i^{(k)}.
\end{equation}

The parent node will perform a value update, but with two differences compared to the normal value update in HGF nodes. First of all, the predicted precision of the binary HGF node will enter the precision update in an unusual way:
\begin{equation}
    \pi_{pa}^{(k)} = \hat{\pi}_{pa}^{(k)} + \frac{1}{\hat{\pi}_i^{(k)}}.
\end{equation}

Second, the \textsf{VAPE} will not be precision-weighted by the low-level precision:
\begin{equation}
    \mu_{pa}^{(k)} = \hat{\mu}_{pa}^{(k)} + \frac{1}{\pi_{pa}^{(k)}} \delta_i^{(k)}.
\end{equation}

For the implementation, this means that we can either give the HGF node knowledge about who its child is and let the exact update depend on that, or we can let this be solved by the value connection, in which case this connection would need to signal the precision weight that is used for the mean update separately from the term that is used for the precision update.\\

In any case, the information which needs to be sent bottom-up by a binary HGF node is:
\begin{description}
    \item[Prediction error:] $\delta_i^{(k)}$
    \item[Predicted precision:] $\hat{\pi}_i^{(k)}$
\end{description}

In case the parent does not have knowledge about its child, the information would have to be sent in the following form:
\begin{description}
    \item[Prediction error:] $\delta_i^{(k)}$
    \item[Precision weight for mean update:] $1$
    \item[Precision term for precision update:] $\frac{1}{\hat{\pi}_i^{(k)}}$
\end{description}

Finally, in the \textsf{PREDICTION step}, the binary HGF node would compute its predictions based on its parents' predictions:
\begin{align}
    \hat{\mu}_i^{(k)} &= \frac{1}{1 + \exp(-\hat{\mu}_{pa}^{(k)})}\\
    \hat{\pi}_i^{(k)} &= \frac{1}{\hat{\mu}_i^{(k)} * (1- \hat{\mu}_i^{(k)})}.
\end{align}

Thus, we need to introduce a new top-down signalling step at the beginning of each trial, where the parent nodes send down its current prediction of the mean, given the time of a new input. We anyway need to do this for the communication with continuous input nodes. \\

\subsubsection{Summary: Implementational consequences}
Due to the special cases of continuous input nodes and binary HGF nodes, which both can be potential children of regular HGF nodes, we need to introduce a few changes to the update and connection logic of regular HGF nodes:
\begin{itemize}
    \item HGF nodes need to elicit new predictions if prompted for by receiving information about the time of the new input, and send this prediction top-down. This is needed both for the computation of surprise in the continuous input nodes, but also for the computation of prediction error in continuous input nodes, and for the computation of predictions in binary HGF nodes.
    \item For the case of value parents of continuous input nodes, HGF nodes need to signal top-down not only their posterior mean, but also their posterior precision, even though this is never needed except in the mentioned case.
    \item Value connections need to separately bottom-up signal the precision weight of the upcoming prediction error, and the precision term needed to update the parent's precision.
    \item Implementing volatility connections in the same way allows for an implementation where regular HGF nodes are completely unaware about which kind of node their child is. The computation necessary for the precision update, which is more elaborate in volatility coupling, will be part of the connection logic. An argument for this could be that the exact wiring of the connections will lead to the excitatory input needed in the level above to perform the precision update.
\end{itemize}

Everything else that is unusual about the computations within binary input nodes, continuous input nodes, and binary HGF nodes will be implemented within these nodes and not affect the regular HGF node.


\section{Relation to Predictive Coding}

In this section, we consider an implementation of the message passing implicated by the HGF which is as close as possible to current proposals of neural architectures for predictive coding \cite{Shipp2016} as shown in figure \ref{fig:shipp}.\\

\loadfigure[fig:shipp]{figures/shipp}

We will separately consider \textsf{VAPE} and \textsf{VOPE} coupling and realize that the message passing for \textsf{VAPE} coupling is almost equivalent to the proposed PC architecture, while the \textsf{VOPE} coupling comes with more challenges, simply because it hasn't been addressed in the same detail in existing PC proposals on neural architecture (but see \cite{Kanai2015}).

\loadfigure[fig:vaall]{figures/vape_all}

\subsection{VAPE coupling}

Figure \ref{fig:vaall} sketches a possible architecture implementing all computations involved in \textsf{VAPE} coupling for three example cortical regions (levels). The assignment of neural elements to cortical layers follows the proposal in figure 3 of \cite{Shipp2016}, reproduced here in figure \ref{fig:shipp}. For example, we've placed all precision-related nodes into the upper layers, while expectations and predictions of the mean live in intermediate and deep layers, respectively. In the following, we go through the different computational steps and note differences to the proposal for predictive coding.

For the \textsf{PE} step, we have the following computation:

\begin{equation}
	\delta_i^{(k)} = \mu_i^{(k)} - \hat{\mu}_i^{(k)}.
\end{equation}

\loadfigure[fig:vape]{figures/vape_pe}

The message passing implicated is depicted in figure \ref{fig:vape} and is in line with PE computations as suggested by \cite{Shipp2016}.

In the \textsf{Update} step, the posterior estimates of mean and variance are given by:

\begin{equation}
	\mu_i^{(k)} = \hat{\mu}_i^{(k)} + \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)}
\end{equation}

and

\begin{equation}
	\pi_i^{(k)} = \hat{\pi}_i^{(k)} + \alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}.
\end{equation}

A possible message passing implementation for these updates is shown in figure \ref{fig:vaupdate}. There are two important differences to \cite{Shipp2016} to note here.\\

\loadfigure[fig:vaupdate]{figures/vape_update}

First, in the \textsf{PC} proposal, the update of the mean $\mu_i$ is driven both by the lower-level PE $\delta_{i-1}$ and the PE on the level itself, $\delta_i$, which mediates the influence of the empirical prior, or the prediction, on the mean. 

In the HGF equations, we do not have this negative feedback loop from $\delta_i$ to $\mu_i$. Instead, the prediction $\hat{\mu}_i$ has a direct influence on $\mu_i$ (see equations above), hence the arrow from the $\hat{\mu}_i$ node to the $\mu_i$ node in figure \ref{fig:vaupdate}. This difference is due to the discrete nature of the update equations in the HGF, and can be resolved by considering the potential (PC-like) within-trial temporal evolution of the $\mu_i$ node. Noting that $\mu_i$ as computed in the above equation will be the endpoint of the update, i.e., the equilibrium value to which the node should stabilize at the end of trial $k$, we can, in the style of \cite{Bogacz2017}, propose the following temporal evolution:

\begin{equation}
	\dot{\mu}_i^{(k)} = \hat{\mu}_i^{(k)} + \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \mu_i^{(k)}
\end{equation}

A node with this dynamic will converge to the required posterior value, which can be seen by setting $\dot{\mu}_i$ to zero. Now we note that

\begin{equation}
	\dot{\mu}_i^{(k)} = \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \mu_i^{(k)} + \hat{\mu}_i^{(k)}
\end{equation}

can be summarized as 

\begin{equation}
	\dot{\mu}_i^{(k)} = \frac{\alpha_{i-1,i}^2 \hat{\pi}_{i-1}^{(k)}}{\pi_i^{(k)}} \delta_{i-1}^{(k)} - \delta_i^{(k)}.
\end{equation}

Following this differential equation for the temporal evolution of node $\mu_i$, we could also place the arrow between $\delta_i$ and $\mu_i$, resulting in the same feedback loop as in the PC proposal.\footnote{Note that in the implementation proposed in this chapter, this would not be possible, as the prediction error is always automatically precision-weighted via the modulation of its dendrites in the superficial layers. For the feedback loop however, one would need the prediction error in its unweighted form, a requirement that will be met in the next chapter.} The exercise of writing down potential within-trial temporal dynamics for all the HGF nodes will be performed and discussed in the next chapter, where we will see that the resulting equations for continuous time network dynamics result in a more detailed and more biologically plausible implementation scheme.

The second difference to the \textsf{PC} proposal is rather irreducible. As already mentioned in the previous section, the messages sent up to parent nodes do not only consist of prediction errors or their precision weights. Instead, to update the precision node $\pi_i$, node $i$ also needs access to the prediction precision $\hat{\pi}_{i-1}$ from the level below. This is reflected in the violet arrow travelling from superficial layers of level $i$ to level $i+1$. However, given that in most \textsf{PC} proposals the update equations were concerned only with hierarchies for estimating the mean and not the precision (but see \cite{Kanai2015} for an exception), it does not seem too surprising that the explicit treatment of precision updates in the HGF results in new predictions for cortical architecture. 

Finally, in the \textsf{Prediction} step, we have:

\begin{equation}
	\hat{\mu}_i^{(k+1)} = \mu_i^{(k)} + \alpha_{i,i+1} \mu_{i+1}^{(k)}
\end{equation}

and

\begin{equation}
	\hat{\pi}_i^{(k+1)} = \frac{1}{\frac{1}{\pi_i^{(k)}} + \exp(\omega_i)}.
\end{equation}

\loadfigure[fig:vaprediction]{figures/vape_prediction}

This again seems unproblematic, although the arrow from the posterior $\mu_i$ to the prediction $\hat{\mu}_i$ would not appear in the classical \textsf{PC} architecture, where the influence of the posterior on the new prediction is again mediated by the prediction error node. This can be resolved by moving to a continuous time representation in the same way as seen before and will be discussed below.\\

Finally, we have placed the predicted precision $\hat{\pi}$ in the superficial layers here, for convenience, as it interacts only with the posterior precision $\pi$ and acts as a weight on the prediction error, and also inaccordance with \cite{Shipp2016}, where precision signals arise from layers 2 and 3A. 

\subsection{VOPE coupling}
For volatility parents, we propose the following: Each level of a cortical hierarchy implements its own volatility parent - or volatility estimation - in subpopulations of neurons within the same cortical column. Coupling across cortical levels, on the other hand, is exclusively value-related. This means that having a volatility parent is nothing more but a more sophisticated way of predicting and updating a level's precision estimates. \\

This setup deliberately excludes volatility parents of volatility parents. However, it allows for a cortical hierarchy where higher levels predict either the mean, or the volatility (or both), of lower levels. In other words, any given level receives top-down predictions not only about its mean, but also about its volatility, the current estimation of which is represented in the mean of its volatility parent. Higher-level predictions of volatility (top-down) and deviations from these predictions - value prediction errors about the volatility mean (bottom-up) are communicated between cortical levels via value (\textsf{VAPE}) coupling in the same way as higher-level predictions of the mean and deviations (value prediction errors about the state mean).\\

!Include schematic visualization of the different scenarios here!\\

Note that this setup also allows for a centralized or global volatility estimation. For example, a cortical or subcortical region which monitors the overall environmental volatility could be coupled to all local volatility units via value coupling. Our proposal is more flexible than previous ones \cite{Kanai2015} though, in that it also enables differential volatility estimation, e.g. within different sensory channels. \\

Moreover, in principle, both the mean $\mu_i$ as well as the volatility estimate $\check{\mu}_i$ of a given level could have (one or more) value parents in higher cortical areas. This allows for both separate and combined higher-level prediction of both the precision and the mean of level $i$.\\

Due to our proposal, we slightly change the notation for volatility coupling from here on. Level $i$ from now on refers to the hierarchical level within a cortical hierarchy. Nodes on this level comprise the 'original' nodes $\mu_i$, $\pi_i$, $\hat{\mu}_i$, $\hat{\pi}_i$, and $\delta_i$, which are concerned with value or state estimation, but also all the nodes belonging to a volatility parent of these nodes, which we now denote as $\check{\mu}_i$, $\check{\pi}_i$, $\hat{\check{\mu}}_i$, $\hat{\check{\pi}}_i$, and $\check{\delta}_i$. Here, $\check{\delta}_i$ refers to the value prediction error about the mean of the volatility, $\check{\mu}_i$. \\

The \textsf{VOPE} $\Delta_i$, which we have written as a function of the \textsf{VAPE} $\delta_i$, of level $i$, is what communicates between a level's nodes and a level's volatility parent's nodes. Value estimation and volatility estimation both happen within a cortical level or column. For the ensuing equations (simple reformulations of the computations in \textsf{VOPE} coupling as presented before) and visualizations of this proposal, please refer to the appendix. \\

!Discuss similarities with and differences to \cite{Kanai2015} here!\\

